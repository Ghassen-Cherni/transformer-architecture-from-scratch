Work in progress, currently implemented:
  - Byte Pair Encoding BPE tokenization
  - Token embedding layer
  - Sinusoidal positional encoding
  - Single-head scaled dot-product attention

Planned components:

- Attention masking (padding and causal)
- Multi-head attention
- Feed-forward network (FFN)
- Layer normalization and residual connections
- Stacking encoder / decoder blocks
- Training loop on a small dataset
