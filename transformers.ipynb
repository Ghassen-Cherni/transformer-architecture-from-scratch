{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41033cf6",
   "metadata": {},
   "source": [
    "## This notebook is an implementation of transformers from scratch for learning purposes - Work in progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07bde849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "# Decided to use the tokenizers library for BPE tokenization to understand how tokenization works under the hood\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ebe0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    def __init__(self):\n",
    "        self.bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        self.bpe_tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.bpe_tokenizer.decoder = ByteLevelDecoder()\n",
    "        self.trainer = BpeTrainer(vocab_size = 50_000, min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"], \n",
    "                                  initial_alphabet=ByteLevel.alphabet())\n",
    "    def train_tokenizer(self, sequences_files: list[str]):\n",
    "        self.bpe_tokenizer.train(sequences_files, self.trainer)\n",
    "        self.bpe_tokenizer.save(\"./tokenizer.json\")\n",
    "     \n",
    "    def encode_word(self, word: str):\n",
    "        output = self.bpe_tokenizer.encode(word)\n",
    "        return output\n",
    "    def embed(self, tokenized_sequence, d_model: int):\n",
    "        ids = tokenized_sequence.ids\n",
    "        return torch.nn.Embedding(self.bpe_tokenizer.get_vocab_size(), d_model)(torch.tensor(ids))\n",
    "        \n",
    "embedding = Embedding()\n",
    "embedding.train_tokenizer([\"./input_text.txt\"])\n",
    "embedded = embedding.encode_word(\"hello world!\")\n",
    "embedded = embedding.embed(embedded, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a3d2e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcea8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make sure to specify the dtype when creating the tensors to avoid casting to float64 and having everything go slower\n",
    "\n",
    "def positional_encoding(embedded):\n",
    "    d_model = embedded.shape[1]\n",
    "    seq_length = embedded.shape[0]\n",
    "    pos = torch.arange(0, seq_length, device=embedded.device, dtype=embedded.dtype).unsqueeze(1) # (seq_length, 1)\n",
    "    i = torch.arange(0, d_model, 2, device=embedded.device, dtype=embedded.dtype) # (d_model/2,)\n",
    "    positional_encoding = torch.zeros_like(embedded)\n",
    "    div_term = torch.exp(-(torch.log(torch.tensor(10_000.0, device=embedded.device, dtype=embedded.dtype)) * (i/d_model)))    \n",
    "    positional_encoding[:, 0::2] = torch.sin(pos/div_term)\n",
    "    positional_encoding[:, 1::2] = torch.cos(pos/div_term)\n",
    "    return embedded + positional_encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5a03cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = positional_encoding(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca814925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2551e+00,  8.7679e-01, -4.5609e-01,  ...,  1.6302e+00,\n",
       "         -2.2152e+00,  8.0549e-01],\n",
       "        [ 2.4942e-01,  1.0974e+00,  4.1486e-01,  ..., -2.0961e+00,\n",
       "          2.0311e+00, -9.3400e-05],\n",
       "        [ 1.8573e+00, -1.2435e-01,  1.9125e+00,  ...,  2.5392e-01,\n",
       "         -2.4414e+00, -9.7617e-01],\n",
       "        ...,\n",
       "        [ 1.2422e+00,  3.3732e-02,  1.5159e+00,  ..., -7.9701e-01,\n",
       "          3.2946e-01, -1.6072e-01],\n",
       "        [ 2.6369e+00, -8.2525e-01,  4.0937e-01,  ...,  8.7939e-02,\n",
       "         -8.4811e-02,  5.5790e-01],\n",
       "        [-2.2704e+00, -1.4055e+00, -6.3212e-01,  ..., -2.3478e+00,\n",
       "          2.0572e+00,  1.7999e+00]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_layer(torch.nn.Module):\n",
    "    def __init__(self, dim_k, dim_q, dim_v, dim_d):\n",
    "        super().__init__()\n",
    "        self.W_Q = torch.randn(dim_d, dim_q)\n",
    "        self.W_K = torch.randn(dim_d, dim_k)\n",
    "        self.W_V = torch.randn(dim_d, dim_v)\n",
    "        self.dim_d = dim_d\n",
    "    def forward(self, x):\n",
    "        Q = x @self.W_Q\n",
    "        K = x @self.W_K\n",
    "        V = x @self.W_V\n",
    "        attention_scores = torch.softmax(Q@K.T/sqrt(self.dim_d))\n",
    "        attention = attention_scores @ V\n",
    "        return attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
