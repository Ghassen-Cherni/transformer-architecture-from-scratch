{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41033cf6",
   "metadata": {},
   "source": [
    "## This notebook is an implementation of transformers from scratch for learning purposes - Work in progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bde849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "# Decided to use the tokenizers library for BPE tokenization to understand how tokenization works under the hood\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ebe0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        self.bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        self.bpe_tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.bpe_tokenizer.decoder = ByteLevelDecoder()\n",
    "        self.trainer = BpeTrainer(vocab_size = 50_000, min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"], \n",
    "                                  initial_alphabet=ByteLevel.alphabet())\n",
    "    def train_tokenizer(self, sequences_files: list[str]):\n",
    "        self.bpe_tokenizer.train(sequences_files, self.trainer)\n",
    "        self.bpe_tokenizer.save(\"./tokenizer.json\")\n",
    "     \n",
    "    def encode_word(self, word: str):\n",
    "        output = self.bpe_tokenizer.encode(word)\n",
    "        return output\n",
    "    def embed(self, tokenized_sequence, d_model: int):\n",
    "        ids = tokenized_sequence.ids\n",
    "        return torch.nn.Embedding(self.bpe_tokenizer.get_vocab_size(), d_model)(torch.tensor(ids))\n",
    "        \n",
    "embedding = Embedding()\n",
    "embedding.train_tokenizer([\"./input_text.txt\"])\n",
    "embedded = embedding.encode_word(\"hello world!\")\n",
    "embedded = embedding.embed(embedded, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a3d2e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2551, -0.1232, -0.4561,  ...,  0.6302, -2.2152, -0.1945],\n",
       "        [-0.5921,  0.5571, -0.4458,  ..., -3.0437,  1.0927,  0.3457],\n",
       "        [ 0.9480,  0.2918,  1.0361,  ..., -0.5419, -1.7925, -0.2153],\n",
       "        ...,\n",
       "        [ 0.2529,  0.1792,  0.6107,  ...,  0.0608,  0.0175,  0.7894],\n",
       "        [ 2.2248,  0.0859,  0.3144,  ...,  1.0642,  0.9147,  0.5261],\n",
       "        [-1.7264, -0.5665,  0.1764,  ..., -1.3539,  1.6781,  0.8746]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dcea8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make sure to specify the dtype when creating the ten  sors to avoid casting to float64 and having everything go slower\n",
    "# Will add max_len_seq later to compare speed\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, embedded):\n",
    "        d_model = embedded.shape[1]\n",
    "        seq_length = embedded.shape[0]\n",
    "        pos = torch.arange(0, seq_length, device=embedded.device, dtype=embedded.dtype).unsqueeze(1) # (seq_length, 1)\n",
    "        i = torch.arange(0, d_model, 2, device=embedded.device, dtype=embedded.dtype) # (d_model/2,)\n",
    "        positional_encoding = torch.zeros_like(embedded)\n",
    "        div_term = torch.exp(-(torch.log(torch.tensor(10_000.0, device=embedded.device, dtype=embedded.dtype)) * (i/d_model)))    \n",
    "        positional_encoding[:, 0::2] = torch.sin(pos/div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(pos/div_term)\n",
    "        return embedded + positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a03cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = positional_encoding(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca814925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_k, dim_q, dim_v, dim_d):\n",
    "        super().__init__()\n",
    "        self.W_Q = torch.nn.Parameter(torch.randn(dim_d, dim_q))\n",
    "        self.W_K = torch.nn.Parameter(torch.randn(dim_d, dim_k))\n",
    "        self.W_V = torch.nn.Parameter(torch.randn(dim_d, dim_v))\n",
    "        self.dim_d = dim_d\n",
    "    def forward(self, x):\n",
    "        Q = x @self.W_Q\n",
    "        K = x @self.W_K\n",
    "        V = x @self.W_V\n",
    "        attention_scores = torch.softmax(Q@K.T/sqrt(self.dim_d))\n",
    "        attention = attention_scores @ V\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_k, dim_q, dim_v, dim_d, num_heads):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, dim_d, dim_q))\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, dim_d, dim_k))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, dim_d, dim_v))\n",
    "        self.dim_d = dim_d\n",
    "        self.scale = sqrt(dim_d)\n",
    "    def forward(self, x):\n",
    "        Q = x @self.W_Q\n",
    "        K = x @self.W_K\n",
    "        V = x @self.W_V\n",
    "        attention_scores = torch.softmax(Q@K.transpose(-2, -1)/self.scale, dim=-1)\n",
    "        attention = attention_scores @ V # (num_heads, seq_length, dim_v)\n",
    "        attention = attention.transpose(0, 1) # (seq_length, num_heads, dim_v)\n",
    "        flattened_attention = attention.reshape(attention.shape[0], -1) \n",
    "        projected_attention = flattened_attention @ nn.Parameter(torch.randn(flattened_attention.shape[1], 512))\n",
    "        return projected_attention, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_head_attention_layer = MultiHeadAttentionLayer(dim_k=512, dim_q=512, dim_v=512, dim_d=512, num_heads=8)\n",
    "output_layer = multi_head_attention_layer.forward(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bee4fc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 512])\n"
     ]
    }
   ],
   "source": [
    "print(output_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba01b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, sublayer_output, previous_layer_output):\n",
    "        return sublayer_output + previous_layer_output\n",
    "\n",
    "\n",
    "\n",
    "# Post Normalization is used here, like in the original Transformer paper\n",
    "class LayerAddAndNormLayer(nn.Module):\n",
    "    def __init__(self, dim_d, e = 1e-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = e\n",
    "        self.dim_d = dim_d\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(self.dim_d))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(self.dim_d))\n",
    "        self.residual = ResidualLayer()\n",
    "    def normalization(self, x):\n",
    "        mean = torch.mean(x, dim = -1, keepdim=True)\n",
    "        variance = torch.var(x, dim = -1, unbiased=False, keepdim=True)\n",
    "        normalized_x = (x-mean)/(torch.sqrt(variance+self.epsilon))\n",
    "        normalized_x = self.gamma * normalized_x + self.beta\n",
    "        return normalized_x \n",
    "    def forward(self, sublayer_output, previous_layer_output):\n",
    "        return self.normalization(self.residual(previous_layer_output, sublayer_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.out_dim = out_dim\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80703914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_d, num_heads):\n",
    "        super().__init__()\n",
    "        self.dim_k = self.dim_v = self.dim_q = dim_d // num_heads\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_q))\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_k))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_v))\n",
    "        self.dim_d = dim_d\n",
    "        self.scale = sqrt(dim_d)\n",
    "        self.W = nn.Parameter(torch.randn(num_heads * self.dim_v, 512))\n",
    "    def forward(self, x):\n",
    "        Q = x @self.W_Q\n",
    "        K = x @self.W_K\n",
    "        V = x @self.W_V\n",
    "        temp = Q@K.transpose(-2, -1)/self.scale\n",
    "        j = torch.arange(0, x.shape[0], device=x.device).unsqueeze(0)\n",
    "        i = torch.arange(0, x.shape[0], device=x.device).unsqueeze(1)\n",
    "        temp = temp.masked_fill(i<j, float('-inf'))\n",
    "        attention_scores = torch.softmax(temp, dim=-1)\n",
    "        attention = attention_scores @ V # (num_heads, seq_length, dim_v)\n",
    "        attention = attention.transpose(0, 1) # (seq_length, num_heads, dim_v)\n",
    "        flattened_attention = attention.reshape(attention.shape[0], -1) \n",
    "        projected_attention = flattened_attention @ self.W\n",
    "        return projected_attention\n",
    "    \n",
    "\n",
    "class MultiHeadCrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, dim_d=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim_k = self.dim_v = self.dim_q = dim_d // num_heads\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_q))\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_k))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_v))\n",
    "        self.dim_d = dim_d\n",
    "        self.scale = sqrt(dim_d)\n",
    "        self.W = nn.Parameter(torch.randn(num_heads * self.dim_v, 512))\n",
    "    def forward(self, x, encoder_output):\n",
    "        Q = x @self.W_Q\n",
    "        K = encoder_output @self.W_K\n",
    "        V = encoder_output @self.W_V\n",
    "        attention_scores = torch.softmax(Q@K.transpose(-2,-1)/self.scale, dim=-1)\n",
    "        attention = attention_scores @ V # (num_heads, seq_length, dim_v)\n",
    "        attention = attention.transpose(0, 1) # (seq_length, num_heads, dim_v)\n",
    "        flattened_attention = attention.reshape(attention.shape[0], -1)\n",
    "        projected_attention = flattened_attention @ self.W\n",
    "        return projected_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.embedding = Embedding()\n",
    "# self.positional_encoding = PositionalEncoding(d_model=dim_d)\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim_d = 512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttentionLayer(dim_d=dim_d, num_heads=num_heads)\n",
    "        self.fully_connected_layer = FullyConnectedLayer(in_dim = dim_d, hidden_dims = [2048], out_dim = dim_d)\n",
    "        self.residual = ResidualLayer()\n",
    "        self.layer_add_and_norm_1 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "        self.layer_add_and_norm_2 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "    def forward(self, x):\n",
    "        attention_output, _, _ = self.multi_head_attention(x)\n",
    "        x = self.layer_add_and_norm_1(attention_output, x)\n",
    "        fc_output = self.fully_connected_layer(x)\n",
    "        x = self.layer_add_and_norm_2(fc_output, x)\n",
    "        return x\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, dim_d = 512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MaskedMultiHeadAttentionLayer(dim_d=dim_d, num_heads=num_heads)\n",
    "        self.layer_add_and_norm_1 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "        self.layer_add_and_norm_2 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "        self.layer_add_and_norm_3 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "        self.multi_head_cross_attention = MultiHeadCrossAttentionLayer( dim_d=dim_d, num_heads=num_heads)\n",
    "        self.fully_connected_layer = FullyConnectedLayer(in_dim = dim_d, hidden_dims = [2048], out_dim = dim_d)\n",
    "    def forward(self, x, encoder_output):\n",
    "        masked_attention_output = self.masked_multi_head_attention(x)\n",
    "        x = self.layer_add_and_norm_1(masked_attention_output, x)\n",
    "        cross_attention_output = self.multi_head_cross_attention(x, encoder_output)\n",
    "        x = self.layer_add_and_norm_2(cross_attention_output, x)\n",
    "        fc_output = self.fully_connected_layer(x)\n",
    "        x = self.layer_add_and_norm_3(fc_output, x)\n",
    "        return x\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_heads, dim_d, num_decoder_layers, vocab_size):\n",
    "        super().__init__()\n",
    "        self.dim_d = dim_d\n",
    "        self.embedding = Embedding()\n",
    "        self.position_encoding = PositionalEncoding(d_model=dim_d)\n",
    "        self.encoder_layers = nn.Sequential(*[EncoderBlock(dim_d=dim_d, num_heads=num_heads) for _ in range(num_encoder_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderBlock(dim_d=dim_d, num_heads=num_heads) for _ in range(num_decoder_layers)])\n",
    "        self.linear_layer = nn.Linear(in_features=dim_d, out_features=vocab_size)\n",
    "        self.softmax_layer = nn.Softmax(dim = -1)\n",
    "    def forward(self, input_sequence, target_sequence):\n",
    "        embedded_input = self.embedding.embed(input_sequence, self.dim_d)\n",
    "        encoded_input = self.position_encoding(embedded_input)\n",
    "        \n",
    "        embedded_output = self.embedding.embed(target_sequence, self.dim_d)\n",
    "        encoded_target = self.position_encoding(embedded_output)\n",
    "\n",
    "        encoder_output = self.encoder_layers(encoded_input)\n",
    "\n",
    "        decoder_output  = encoded_target\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output , encoder_output)\n",
    "\n",
    "        output_linear = self.linear_layer(decoder_output)\n",
    "        final_output = self.softmax_layer(output_linear)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470028a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
