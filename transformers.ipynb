{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41033cf6",
   "metadata": {},
   "source": [
    "## This notebook is an implementation of transformers from scratch for learning purposes - Work in progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07bde849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "# Decided to use the tokenizers library for BPE tokenization to understand how tokenization works under the hood\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed81b5",
   "metadata": {},
   "source": [
    "### Embedding with Byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BpeTokenizer():\n",
    "    def __init__(self, vocab_size = 32_000):\n",
    "        self.bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))  # With byte-level BPE, [UNK] should be rare\n",
    "        self.bpe_tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.bpe_tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "        self.trainer = BpeTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=2,\n",
    "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n",
    "            initial_alphabet=ByteLevel.alphabet(),\n",
    "        )\n",
    "        self.pad_id = None\n",
    "        self.unk_id = None\n",
    "        self.bos_id = None\n",
    "        self.eos_id = None\n",
    "\n",
    "    def train(self, input_text_paths, output_tokenizer_path):\n",
    "        self.bpe_tokenizer.train(input_text_paths, self.trainer)\n",
    "        self.bpe_tokenizer.save(output_tokenizer_path)\n",
    "        self.cache_special_tokens()\n",
    "\n",
    "    def load(self, tokenizer_path):\n",
    "        self.bpe_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        self.cache_special_tokens()\n",
    "    \n",
    "    def cache_special_tokens(self):\n",
    "        self.pad_id = self.bpe_tokenizer.token_to_id(\"[PAD]\")\n",
    "        self.unk_id = self.bpe_tokenizer.token_to_id(\"[UNK]\")\n",
    "        self.bos_id = self.bpe_tokenizer.token_to_id(\"[BOS]\")\n",
    "        self.eos_id = self.bpe_tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "    def encode_input(self, text):\n",
    "        return self.bpe_tokenizer.encode(text).ids\n",
    "    def encode_encoder_input(self, text):\n",
    "        encoding = self.bpe_tokenizer.encode(text).ids + [self.EOS_ID]\n",
    "        return encoding\n",
    "    def encode_decoder_input(self, text):\n",
    "        encoding = [self.BOS_ID] + self.bpe_tokenizer.encode(text).ids \n",
    "        return encoding\n",
    "    def encode_target(self, text):\n",
    "        encoding = self.bpe_tokenizer.encode(text).ids + [self.EOS_ID]\n",
    "        return encoding\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return self.bpe_tokenizer.decode(ids, skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  \n",
    "\n",
    "    def forward(self, ids):\n",
    "        return self.embedding(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d2e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "# Just testing here\n",
    "train_bpe_tokenizer(\"./input_text.txt\", \"./tokenizer.json\", vocab_size=32_000)\n",
    "\n",
    "emb = TokenEmbedding(\"./tokenizer.json\", d_model=512)\n",
    "ids = emb.encode(\"hello world!\")\n",
    "x = emb(ids)  \n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "12dc1ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([224,  75, 269,  79,  82, 224,  90, 262,  79,  71,   4])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = emb.encode(\"hello world!\")\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556f5b6",
   "metadata": {},
   "source": [
    "### Sinusoidal Positional encoding to add position info to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dcea8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make sure to specify the dtype when creating the ten  sors to avoid casting to float64 and having everything go slower\n",
    "# Will add max_len_seq later to compare speed\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, embedded):\n",
    "        d_model = embedded.shape[1]\n",
    "        seq_length = embedded.shape[0]\n",
    "        pos = torch.arange(0, seq_length, device=embedded.device, dtype=embedded.dtype).unsqueeze(1) # (seq_length, 1)\n",
    "        i = torch.arange(0, d_model, 2, device=embedded.device, dtype=embedded.dtype) # (d_model/2,)\n",
    "        positional_encoding = torch.zeros_like(embedded)\n",
    "        div_term = torch.exp(-(torch.log(torch.tensor(10_000.0, device=embedded.device, dtype=embedded.dtype)) * (i/d_model)))    \n",
    "        positional_encoding[:, 0::2] = torch.sin(pos/div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(pos/div_term)\n",
    "        return embedded + positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5a03cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = positional_encoding(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca814925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 512])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389fc61",
   "metadata": {},
   "source": [
    "### Simple attention layer (To understand the concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a39d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_k, dim_q, dim_v, dim_d):\n",
    "        super().__init__()\n",
    "        self.W_Q = torch.nn.Parameter(torch.randn(dim_d, dim_q))\n",
    "        self.W_K = torch.nn.Parameter(torch.randn(dim_d, dim_k))\n",
    "        self.W_V = torch.nn.Parameter(torch.randn(dim_d, dim_v))\n",
    "        self.dim_d = dim_d\n",
    "    def forward(self, x):\n",
    "        Q = x @self.W_Q\n",
    "        K = x @self.W_K\n",
    "        V = x @self.W_V\n",
    "        attention_scores = torch.softmax(Q@K.T/sqrt(self.dim_d))\n",
    "        attention = attention_scores @ V\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5c10c",
   "metadata": {},
   "source": [
    "### Upping the level here, added multi head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b119e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_k, dim_q, dim_v, dim_d, num_heads):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, dim_d, dim_q))\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, dim_d, dim_k))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, dim_d, dim_v))\n",
    "        self.dim_d = dim_d\n",
    "        self.scale = sqrt(dim_d)\n",
    "    def forward(self, x):\n",
    "        Q = x @self.W_Q\n",
    "        K = x @self.W_K\n",
    "        V = x @self.W_V\n",
    "        attention_scores = torch.softmax(Q@K.transpose(-2, -1)/self.scale, dim=-1)\n",
    "        attention = attention_scores @ V # (num_heads, seq_length, dim_v)\n",
    "        attention = attention.transpose(0, 1) # (seq_length, num_heads, dim_v)\n",
    "        flattened_attention = attention.reshape(attention.shape[0], -1) \n",
    "        projected_attention = flattened_attention @ nn.Parameter(torch.randn(flattened_attention.shape[1], 512))\n",
    "        return projected_attention, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "98bb9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_head_attention_layer = MultiHeadAttentionLayer(dim_k=512, dim_q=512, dim_v=512, dim_d=512, num_heads=8)\n",
    "output_layer = multi_head_attention_layer.forward(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bee4fc9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(output_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c024a",
   "metadata": {},
   "source": [
    "### Add & Norm Layer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba01b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, sublayer_output, previous_layer_output):\n",
    "        return sublayer_output + previous_layer_output\n",
    "\n",
    "\n",
    "\n",
    "# Post Normalization is used here, like in the original Transformer paper\n",
    "class LayerAddAndNormLayer(nn.Module):\n",
    "    def __init__(self, dim_d, e = 1e-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = e\n",
    "        self.dim_d = dim_d\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(self.dim_d))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(self.dim_d))\n",
    "        self.residual = ResidualLayer()\n",
    "    def normalization(self, x):\n",
    "        mean = torch.mean(x, dim = -1, keepdim=True)\n",
    "        variance = torch.var(x, dim = -1, unbiased=False, keepdim=True)\n",
    "        normalized_x = (x-mean)/(torch.sqrt(variance+self.epsilon))\n",
    "        normalized_x = self.gamma * normalized_x + self.beta\n",
    "        return normalized_x \n",
    "    def forward(self, sublayer_output, previous_layer_output):\n",
    "        return self.normalization(self.residual(previous_layer_output, sublayer_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f63d5",
   "metadata": {},
   "source": [
    "### Position-wise fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.out_dim = out_dim\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55b089",
   "metadata": {},
   "source": [
    "### Well.. we need a Masked MHA and a cross-attention MHA now for the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80703914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_d, num_heads):\n",
    "        super().__init__()\n",
    "        self.dim_k = self.dim_v = self.dim_q = dim_d // num_heads\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_q))\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_k))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_v))\n",
    "        self.dim_d = dim_d\n",
    "        self.scale = sqrt(dim_d)\n",
    "        self.W = nn.Parameter(torch.randn(num_heads * self.dim_v, 512))\n",
    "    def forward(self, x):\n",
    "        Q = x @self.W_Q\n",
    "        K = x @self.W_K\n",
    "        V = x @self.W_V\n",
    "        temp = Q@K.transpose(-2, -1)/self.scale\n",
    "        j = torch.arange(0, x.shape[0], device=x.device).unsqueeze(0)\n",
    "        i = torch.arange(0, x.shape[0], device=x.device).unsqueeze(1)\n",
    "        temp = temp.masked_fill(i<j, float('-inf'))\n",
    "        attention_scores = torch.softmax(temp, dim=-1)\n",
    "        attention = attention_scores @ V # (num_heads, seq_length, dim_v)\n",
    "        attention = attention.transpose(0, 1) # (seq_length, num_heads, dim_v)\n",
    "        flattened_attention = attention.reshape(attention.shape[0], -1) \n",
    "        projected_attention = flattened_attention @ self.W\n",
    "        return projected_attention\n",
    "    \n",
    "\n",
    "class MultiHeadCrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, dim_d=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim_k = self.dim_v = self.dim_q = dim_d // num_heads\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_q))\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_k))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, dim_d, self.dim_v))\n",
    "        self.dim_d = dim_d\n",
    "        self.scale = sqrt(dim_d)\n",
    "        self.W = nn.Parameter(torch.randn(num_heads * self.dim_v, 512))\n",
    "    def forward(self, x, encoder_output):\n",
    "        Q = x @self.W_Q\n",
    "        K = encoder_output @self.W_K\n",
    "        V = encoder_output @self.W_V\n",
    "        attention_scores = torch.softmax(Q@K.transpose(-2,-1)/self.scale, dim=-1)\n",
    "        attention = attention_scores @ V # (num_heads, seq_length, dim_v)\n",
    "        attention = attention.transpose(0, 1) # (seq_length, num_heads, dim_v)\n",
    "        flattened_attention = attention.reshape(attention.shape[0], -1)\n",
    "        projected_attention = flattened_attention @ self.W\n",
    "        return projected_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.embedding = Embedding()\n",
    "# self.positional_encoding = PositionalEncoding(d_model=dim_d)\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim_d = 512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttentionLayer(dim_d=dim_d, num_heads=num_heads)\n",
    "        self.fully_connected_layer = FullyConnectedLayer(in_dim = dim_d, hidden_dims = [2048], out_dim = dim_d)\n",
    "        self.residual = ResidualLayer()\n",
    "        self.layer_add_and_norm_1 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "        self.layer_add_and_norm_2 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "    def forward(self, x):\n",
    "        attention_output, _, _ = self.multi_head_attention(x)\n",
    "        x = self.layer_add_and_norm_1(attention_output, x)\n",
    "        fc_output = self.fully_connected_layer(x)\n",
    "        x = self.layer_add_and_norm_2(fc_output, x)\n",
    "        return x\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, dim_d = 512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MaskedMultiHeadAttentionLayer(dim_d=dim_d, num_heads=num_heads)\n",
    "        self.layer_add_and_norm_1 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "        self.layer_add_and_norm_2 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "        self.layer_add_and_norm_3 = LayerAddAndNormLayer(dim_d=dim_d)\n",
    "        self.multi_head_cross_attention = MultiHeadCrossAttentionLayer( dim_d=dim_d, num_heads=num_heads)\n",
    "        self.fully_connected_layer = FullyConnectedLayer(in_dim = dim_d, hidden_dims = [2048], out_dim = dim_d)\n",
    "    def forward(self, x, encoder_output):\n",
    "        masked_attention_output = self.masked_multi_head_attention(x)\n",
    "        x = self.layer_add_and_norm_1(masked_attention_output, x)\n",
    "        cross_attention_output = self.multi_head_cross_attention(x, encoder_output)\n",
    "        x = self.layer_add_and_norm_2(cross_attention_output, x)\n",
    "        fc_output = self.fully_connected_layer(x)\n",
    "        x = self.layer_add_and_norm_3(fc_output, x)\n",
    "        return x\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ca92a",
   "metadata": {},
   "source": [
    "### Full transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_heads, dim_d, num_decoder_layers, vocab_size):\n",
    "        super().__init__()\n",
    "        self.dim_d = dim_d\n",
    "        self.embedding = Embedding()\n",
    "        self.position_encoding = PositionalEncoding(d_model=dim_d)\n",
    "        self.encoder_layers = nn.Sequential(*[EncoderBlock(dim_d=dim_d, num_heads=num_heads) for _ in range(num_encoder_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderBlock(dim_d=dim_d, num_heads=num_heads) for _ in range(num_decoder_layers)])\n",
    "        self.linear_layer = nn.Linear(in_features=dim_d, out_features=vocab_size)\n",
    "        self.softmax_layer = nn.Softmax(dim = -1)\n",
    "    def forward(self, input_sequence, target_sequence):\n",
    "        embedded_input = self.embedding.embed(input_sequence, self.dim_d)\n",
    "        encoded_input = self.position_encoding(embedded_input)\n",
    "        \n",
    "        embedded_output = self.embedding.embed(target_sequence, self.dim_d)\n",
    "        encoded_target = self.position_encoding(embedded_output)\n",
    "\n",
    "        encoder_output = self.encoder_layers(encoded_input)\n",
    "\n",
    "        decoder_output  = encoded_target\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output , encoder_output)\n",
    "\n",
    "        output_linear = self.linear_layer(decoder_output)\n",
    "        final_output = self.softmax_layer(output_linear)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7f0e4",
   "metadata": {},
   "source": [
    "### Now we prepare our data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e8240adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"[UNK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"[BOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":3, \"content\":\"[EOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), post_processor=None, decoder=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), model=BPE(dropout=None, unk_token=\"[UNK]\", continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"[PAD]\":0, \"[UNK]\":1, \"[BOS]\":2, \"[EOS]\":3, \"!\":4, \"\"\":5, \"#\":6, \"$\":7, \"%\":8, \"&\":9, \"'\":10, \"(\":11, \")\":12, \"*\":13, \"+\":14, \",\":15, \"-\":16, \".\":17, \"/\":18, \"0\":19, \"1\":20, \"2\":21, \"3\":22, \"4\":23, \"5\":24, \"6\":25, \"7\":26, \"8\":27, \"9\":28, \":\":29, \";\":30, \"<\":31, \"=\":32, \">\":33, \"?\":34, \"@\":35, \"A\":36, \"B\":37, \"C\":38, \"D\":39, \"E\":40, \"F\":41, \"G\":42, \"H\":43, \"I\":44, \"J\":45, \"K\":46, \"L\":47, \"M\":48, \"N\":49, \"O\":50, \"P\":51, \"Q\":52, \"R\":53, \"S\":54, \"T\":55, \"U\":56, \"V\":57, \"W\":58, \"X\":59, \"Y\":60, \"Z\":61, \"[\":62, \"\\\":63, \"]\":64, \"^\":65, \"_\":66, \"`\":67, \"a\":68, \"b\":69, \"c\":70, \"d\":71, \"e\":72, \"f\":73, \"g\":74, \"h\":75, \"i\":76, \"j\":77, \"k\":78, \"l\":79, \"m\":80, \"n\":81, \"o\":82, \"p\":83, \"q\":84, \"r\":85, \"s\":86, \"t\":87, \"u\":88, \"v\":89, \"w\":90, \"x\":91, \"y\":92, \"z\":93, \"{\":94, \"|\":95, \"}\":96, \"~\":97, \"¡\":98, ...}, merges=[(\"Ġ\", \"a\"), (\"i\", \"n\"), (\"a\", \"n\"), (\"Ġ\", \"d\"), (\"Ġ\", \"s\"), (\"u\", \"n\"), (\"e\", \"s\"), (\"e\", \"n\"), (\"Ġ\", \"p\"), (\"Ġ\", \"t\"), (\"Ġ\", \"b\"), (\"o\", \"n\"), (\"e\", \"r\"), (\"Ġ\", \"c\"), (\"in\", \"g\"), (\"Ġ\", \"un\"), (\"l\", \"e\"), (\"r\", \"e\"), (\"o\", \"u\"), (\"Ġ\", \"w\"), (\"i\", \"s\"), (\"a\", \"r\"), (\"Ġ\", \"m\"), (\"Ġ\", \"f\"), (\"i\", \"t\"), (\"Ġ\", \"h\"), (\"Ġ\", \"l\"), (\"Ġ\", \"U\"), (\"ĠU\", \"n\"), (\"o\", \"m\"), (\"Ġd\", \"e\"), (\"Ġ\", \"A\"), (\"Ġ\", \"g\"), (\"Ã\", \"©\"), (\"e\", \"t\"), (\"h\", \"e\"), (\"Ġ\", \"in\"), (\"an\", \"t\"), (\"u\", \"r\"), (\"o\", \"r\"), (\"m\", \"e\"), (\"i\", \"r\"), (\"a\", \"l\"), (\"Ġ\", \"r\"), (\"Ġ\", \"en\"), (\"a\", \"t\"), (\"i\", \"l\"), (\"Ġun\", \"e\"), (\"en\", \"t\"), (\"Ġ\", \"o\"), (\"Ġt\", \"he\"), (\"Ġ\", \"j\"), (\"Ġa\", \"n\"), (\"v\", \"e\"), (\"u\", \"e\"), (\"Ġh\", \"om\"), (\"Ġ\", \"on\"), (\"an\", \"s\"), (\"Ġ\", \"v\"), (\"a\", \"c\"), (\"r\", \"o\"), (\"Ġs\", \"ur\"), (\"Ġd\", \"ans\"), (\"e\", \"m\"), (\"a\", \"s\"), (\"Ġm\", \"an\"), (\"Ġ\", \"T\"), (\"Ġhom\", \"me\"), (\"a\", \"u\"), (\"Ġ\", \"is\"), (\"er\", \"s\"), (\"t\", \"e\"), (\"Ġo\", \"f\"), (\"Ġ\", \"et\"), (\"g\", \"e\"), (\"Ġan\", \"d\"), (\"an\", \"d\"), (\"Ġb\", \"l\"), (\"Ġs\", \"t\"), (\"Ġa\", \"ve\"), (\"Ġave\", \"c\"), (\"Ġc\", \"h\"), (\"Ġl\", \"a\"), (\"Ġ\", \"n\"), (\"Ġd\", \"es\"), (\"i\", \"c\"), (\"it\", \"h\"), (\"Ġw\", \"ith\"), (\"ĠUn\", \"e\"), (\"Ġg\", \"r\"), (\"o\", \"l\"), (\"Ġ\", \"Ã\"), (\"ou\", \"r\"), (\"o\", \"t\"), (\"u\", \"x\"), (\"Ġ\", \"re\"), (\"Ġf\", \"em\"), (\"d\", \"e\"), (\"c\", \"h\"), ...]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_path = \"./dataset\"\n",
    "french_dataset_path = \"./dataset/train_fr.txt\"\n",
    "english_dataset_path = \"./dataset/train_en.txt\"\n",
    "\n",
    "\n",
    "with open(french_dataset_path, 'r', encoding='utf-8') as f:\n",
    "    french_lines = f.readlines()\n",
    "train_size = int(len(french_lines) * 0.8)\n",
    "french_lines_train = french_lines[:train_size]\n",
    "french_lines_val = french_lines[train_size:]\n",
    "\n",
    "with open(english_dataset_path, 'r', encoding='utf-8') as f:\n",
    "    english_lines = f.readlines()\n",
    "english_lines_train = english_lines[:train_size]\n",
    "english_lines_val = english_lines[train_size:]\n",
    "\n",
    "\n",
    "all_train_sentences = french_lines_train + english_lines_train\n",
    "\n",
    "# Creating our vocab \n",
    "train_bpe_tokenizer([french_dataset_path, english_dataset_path], \"./tokenizer.json\", vocab_size=32_000)\n",
    "\n",
    "# emb = TokenEmbedding(\"./tokenizer.json\", d_model=512)\n",
    "# ids = emb.encode(\"hello world!\")\n",
    "# x = emb(ids)  \n",
    "# print(ids.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b8e63e76",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TokenEmbedding' object has no attribute 'EOS_ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEOS_ID\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chern\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TokenEmbedding' object has no attribute 'EOS_ID'"
     ]
    }
   ],
   "source": [
    "emb.EOS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209bec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([224,  75, 269,  79,  82, 224,  90, 262,  79,  71,   4])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9dc061fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IWSLTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, french_sentences, english_sentences, tokenizer_path):\n",
    "        self.french_sentences = french_sentences\n",
    "        self.english_sentences = english_sentences\n",
    "        self.tokenizer = TokenEmbedding.from_file(tokenizer_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.french_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        french_sentence = self.french_sentences[idx]\n",
    "        english_sentence = self.english_sentences[idx]\n",
    "        \n",
    "        # We're translating from french to english, so the encoder input is going to be french\n",
    "        french_sentence = french_sentence + \"[EOS]\" \n",
    "        french_ids = self.tokenizer.encode(french_sentence)\n",
    "        french_embeddings = self.tokenizer.forward(french_ids)\n",
    "\n",
    "        english_sentence_decoder_input = \"[BOS]\" + english_sentence\n",
    "        english_sentence_target = english_sentence + \"[EOS]\"\n",
    "\n",
    "        english_ids_decoder_input = self.tokenizer.encode(english_sentence_decoder_input)\n",
    "        english_ids_target = self.tokenizer.encode(english_sentence_target)\n",
    "        \n",
    "        english_embeddings_decoder_input = self.tokenizer.forward(english_ids_decoder_input)    \n",
    "\n",
    "        \n",
    "        return french_embeddings, english_embeddings_decoder_input, english_ids_target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
